{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465a90d9",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's install dependencies and import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ca7c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if running in Colab)\n",
    "# !pip install timm==0.6.13 transformers==4.29.2 albumentations==1.3.1 yacs==0.1.8\n",
    "# !pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style (using compatible style)\n",
    "try:\n",
    "    plt.style.use('seaborn-darkgrid')\n",
    "except:\n",
    "    plt.style.use('default')\n",
    "    \n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7119dc9c",
   "metadata": {},
   "source": [
    "## 2. Download Model and Setup Paths\n",
    "\n",
    "For Colab users: Upload the model checkpoint and sample data, or use the provided download links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752c6983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Configuration\n",
    "MODEL_PATH = '../experiments/training_20251119-095340/20251119-095356/models/best_model_97.pth'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# If running in Colab, uncomment and modify:\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# MODEL_PATH = '/content/drive/MyDrive/DermFormer/best_model_97.pth'\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Model path: {MODEL_PATH}\")\n",
    "print(f\"Model exists: {os.path.exists(MODEL_PATH)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d119e6ea",
   "metadata": {},
   "source": [
    "## 3. Define DermFormer Architecture\n",
    "\n",
    "The model architecture consists of:\n",
    "1. **NeST Backbones**: Separate encoders for clinical and dermoscopic images\n",
    "2. **TabTransformer**: Processes patient metadata\n",
    "3. **Cross-Attention Fusion**: Integrates information across modalities\n",
    "4. **Entropy-Weighted Ensemble**: Combines predictions based on confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e241ae1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add model code path\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from models.DermFormer import DermFormer\n",
    "from models.NesT.nest_cli import nest_cli\n",
    "from models.NesT.nest_der import nest_der\n",
    "from models.NesT.nest_multimodalconcat import nest_MMC\n",
    "\n",
    "# Initialize model\n",
    "model = DermFormer(num_classes=5, hidden_dim=256).to(DEVICE)\n",
    "\n",
    "# Load trained weights\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model.eval()\n",
    "    print(\"Model loaded successfully!\")\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "else:\n",
    "    print(\"Model checkpoint not found. Please upload the model file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc66ea7",
   "metadata": {},
   "source": [
    "## 4. Preprocessing Pipeline\n",
    "\n",
    "Define image transformations and metadata encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcaec81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image preprocessing\n",
    "transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Metadata encodings (from Derm7pt dataset)\n",
    "METADATA_ENCODINGS = {\n",
    "    'elevation': {'flat': 0, 'raised': 1, 'unknown': 2},\n",
    "    'sex': {'male': 0, 'female': 1, 'unknown': 2},\n",
    "    'location': {\n",
    "        'back': 0, 'lower extremity': 1, 'abdomen': 2, 'face': 3,\n",
    "        'chest': 4, 'upper extremity': 5, 'scalp': 6, 'neck': 7, 'unknown': 8\n",
    "    },\n",
    "    'lesion_type': {'typical': 0, 'atypical': 1},\n",
    "    'age_group': {'<30': 0, '30-50': 1, '>50': 2}\n",
    "}\n",
    "\n",
    "DIAGNOSIS_LABELS = {\n",
    "    0: 'Basal Cell Carcinoma',\n",
    "    1: 'Nevus',\n",
    "    2: 'Melanoma',\n",
    "    3: 'Miscellaneous',\n",
    "    4: 'Seborrheic Keratosis'\n",
    "}\n",
    "\n",
    "TASK_LABELS = {\n",
    "    'diag': 'Diagnosis',\n",
    "    'pn': 'Pigment Network',\n",
    "    'bwv': 'Blue-Whitish Veil',\n",
    "    'vs': 'Vascular Structures',\n",
    "    'pig': 'Pigmentation',\n",
    "    'str': 'Streaks',\n",
    "    'dag': 'Dots and Globules',\n",
    "    'rs': 'Regression Structures'\n",
    "}\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"Load and preprocess an image.\"\"\"\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img_np = np.array(img)\n",
    "    transformed = transform(image=img_np)\n",
    "    return transformed['image'], img_np\n",
    "\n",
    "def encode_metadata(elevation='unknown', sex='unknown', location='unknown', \n",
    "                    lesion_type='typical', age_group='30-50'):\n",
    "    \"\"\"Encode metadata into categorical tensor.\"\"\"\n",
    "    meta_cat = torch.tensor([\n",
    "        METADATA_ENCODINGS['elevation'].get(elevation, 2),\n",
    "        METADATA_ENCODINGS['sex'].get(sex, 2),\n",
    "        METADATA_ENCODINGS['location'].get(location, 8),\n",
    "        METADATA_ENCODINGS['lesion_type'].get(lesion_type, 0),\n",
    "        METADATA_ENCODINGS['age_group'].get(age_group, 1)\n",
    "    ]).unsqueeze(0).long()\n",
    "    meta_con = torch.empty((1, 0)).long()\n",
    "    return meta_cat, meta_con\n",
    "\n",
    "print(\"Preprocessing pipeline ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e85e98f",
   "metadata": {},
   "source": [
    "## 5. Inference Function\n",
    "\n",
    "Run the model and extract all predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5826831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(clinical_img_path, dermoscopic_img_path, metadata_dict):\n",
    "    \"\"\"\n",
    "    Run inference on a skin lesion case.\n",
    "    \n",
    "    Args:\n",
    "        clinical_img_path: Path to clinical image\n",
    "        dermoscopic_img_path: Path to dermoscopic image\n",
    "        metadata_dict: Dictionary with keys: elevation, sex, location, lesion_type, age_group\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing predictions for all tasks and branches\n",
    "    \"\"\"\n",
    "    # Preprocess images\n",
    "    cli_tensor, cli_img = preprocess_image(clinical_img_path)\n",
    "    der_tensor, der_img = preprocess_image(dermoscopic_img_path)\n",
    "    \n",
    "    # Encode metadata\n",
    "    meta_cat, meta_con = encode_metadata(**metadata_dict)\n",
    "    \n",
    "    # Move to device\n",
    "    cli_tensor = cli_tensor.unsqueeze(0).to(DEVICE)\n",
    "    der_tensor = der_tensor.unsqueeze(0).to(DEVICE)\n",
    "    meta_cat = meta_cat.to(DEVICE)\n",
    "    meta_con = meta_con.to(DEVICE)\n",
    "    \n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(meta_cat, meta_con, cli_tensor, der_tensor)\n",
    "    \n",
    "    # Parse outputs\n",
    "    results = {\n",
    "        'images': {'clinical': cli_img, 'dermoscopic': der_img},\n",
    "        'metadata': metadata_dict,\n",
    "        'predictions': {}\n",
    "    }\n",
    "    \n",
    "    task_keys = ['diag', 'pn', 'bwv', 'vs', 'pig', 'str', 'dag', 'rs']\n",
    "    branch_names = ['Clinical', 'Dermoscopic', 'Combined', 'Meta-Combined']\n",
    "    \n",
    "    for task_key in task_keys:\n",
    "        task_output = outputs[task_key]\n",
    "        \n",
    "        # Extract branch logits (indices 0-3)\n",
    "        branch_logits = [task_output[i].cpu() for i in range(4)]\n",
    "        branch_probs = [F.softmax(logits, dim=1).squeeze().numpy() for logits in branch_logits]\n",
    "        branch_preds = [probs.argmax() for probs in branch_probs]\n",
    "        \n",
    "        # Extract ensemble prediction (last element)\n",
    "        ensemble_probs = task_output[-2].cpu().squeeze().numpy()\n",
    "        ensemble_pred = task_output[-1].cpu().item()\n",
    "        \n",
    "        results['predictions'][task_key] = {\n",
    "            'branches': {\n",
    "                branch_names[i]: {\n",
    "                    'probabilities': branch_probs[i],\n",
    "                    'prediction': branch_preds[i]\n",
    "                } for i in range(4)\n",
    "            },\n",
    "            'ensemble': {\n",
    "                'probabilities': ensemble_probs,\n",
    "                'prediction': ensemble_pred\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Inference function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd055067",
   "metadata": {},
   "source": [
    "## 6. Visualization Functions\n",
    "\n",
    "Create beautiful visualizations of model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45286457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prediction(results, task='diag', figsize=(16, 10)):\n",
    "    \"\"\"\n",
    "    Visualize predictions for a specific task.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # Display images\n",
    "    ax1 = fig.add_subplot(gs[0, 0:2])\n",
    "    ax1.imshow(results['images']['clinical'])\n",
    "    ax1.set_title('Clinical Image', fontsize=14, fontweight='bold')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    ax2 = fig.add_subplot(gs[0, 2:4])\n",
    "    ax2.imshow(results['images']['dermoscopic'])\n",
    "    ax2.set_title('Dermoscopic Image', fontsize=14, fontweight='bold')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    # Display metadata\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    ax3.axis('off')\n",
    "    metadata_text = \"Patient Metadata:\\n\\n\"\n",
    "    for key, value in results['metadata'].items():\n",
    "        metadata_text += f\"{key.replace('_', ' ').title()}: {value}\\n\"\n",
    "    ax3.text(0.1, 0.5, metadata_text, fontsize=11, verticalalignment='center',\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    # Branch predictions comparison\n",
    "    pred_data = results['predictions'][task]\n",
    "    branch_names = list(pred_data['branches'].keys())\n",
    "    \n",
    "    ax4 = fig.add_subplot(gs[1, 1:4])\n",
    "    x_pos = np.arange(len(branch_names))\n",
    "    confidences = [pred_data['branches'][b]['probabilities'].max() for b in branch_names]\n",
    "    colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
    "    \n",
    "    bars = ax4.bar(x_pos, confidences, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "    ax4.set_ylabel('Confidence', fontsize=12, fontweight='bold')\n",
    "    ax4.set_title(f'{TASK_LABELS[task]} - Branch Confidences', fontsize=13, fontweight='bold')\n",
    "    ax4.set_xticks(x_pos)\n",
    "    ax4.set_xticklabels(branch_names, rotation=15, ha='right')\n",
    "    ax4.set_ylim([0, 1])\n",
    "    ax4.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, conf in zip(bars, confidences):\n",
    "        height = bar.get_height()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{conf:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Ensemble prediction probabilities\n",
    "    ax5 = fig.add_subplot(gs[2, :])\n",
    "    ensemble_probs = pred_data['ensemble']['probabilities']\n",
    "    ensemble_pred = pred_data['ensemble']['prediction']\n",
    "    \n",
    "    if task == 'diag':\n",
    "        labels = [DIAGNOSIS_LABELS[i] for i in range(len(ensemble_probs))]\n",
    "    else:\n",
    "        labels = [f'Class {i}' for i in range(len(ensemble_probs))]\n",
    "    \n",
    "    bars = ax5.barh(labels, ensemble_probs, color='steelblue', alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "    bars[ensemble_pred].set_color('#27ae60')\n",
    "    bars[ensemble_pred].set_alpha(1.0)\n",
    "    bars[ensemble_pred].set_linewidth(3)\n",
    "    \n",
    "    ax5.set_xlabel('Probability', fontsize=12, fontweight='bold')\n",
    "    ax5.set_title(f'Entropy-Weighted Ensemble Prediction: {labels[ensemble_pred]} ({ensemble_probs[ensemble_pred]:.1%})',\n",
    "                  fontsize=13, fontweight='bold', color='#27ae60')\n",
    "    ax5.set_xlim([0, 1])\n",
    "    ax5.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add probability values\n",
    "    for i, (bar, prob) in enumerate(zip(bars, ensemble_probs)):\n",
    "        ax5.text(prob, bar.get_y() + bar.get_height()/2,\n",
    "                f' {prob:.3f}', va='center', fontweight='bold' if i == ensemble_pred else 'normal')\n",
    "    \n",
    "    plt.suptitle(f'DermFormer Multi-Modal Analysis: {TASK_LABELS[task]}',\n",
    "                 fontsize=16, fontweight='bold', y=0.98)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def visualize_all_tasks(results, figsize=(18, 12)):\n",
    "    \"\"\"\n",
    "    Visualize predictions for all 8 tasks in a grid.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 4, figsize=figsize)\n",
    "    fig.suptitle('DermFormer: All Task Predictions', fontsize=18, fontweight='bold', y=0.98)\n",
    "    \n",
    "    task_keys = ['diag', 'pn', 'bwv', 'vs', 'pig', 'str', 'dag', 'rs']\n",
    "    \n",
    "    for idx, (task_key, ax) in enumerate(zip(task_keys, axes.flat)):\n",
    "        pred_data = results['predictions'][task_key]\n",
    "        ensemble_probs = pred_data['ensemble']['probabilities']\n",
    "        ensemble_pred = pred_data['ensemble']['prediction']\n",
    "        \n",
    "        # Create bar plot\n",
    "        x_pos = np.arange(len(ensemble_probs))\n",
    "        bars = ax.bar(x_pos, ensemble_probs, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "        bars[ensemble_pred].set_color('#27ae60')\n",
    "        bars[ensemble_pred].set_alpha(1.0)\n",
    "        \n",
    "        ax.set_title(f'{TASK_LABELS[task_key]}\\nPrediction: Class {ensemble_pred} ({ensemble_probs[ensemble_pred]:.1%})',\n",
    "                    fontsize=10, fontweight='bold')\n",
    "        ax.set_ylabel('Probability', fontsize=9)\n",
    "        ax.set_xlabel('Class', fontsize=9)\n",
    "        ax.set_xticks(x_pos)\n",
    "        ax.set_ylim([0, 1])\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "print(\"âœ… Visualization functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ffb4ca",
   "metadata": {},
   "source": [
    "## 7. Demo: Run Inference on Sample Cases\n",
    "\n",
    "Now let's test the model on real examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a486470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load demo cases from extracted metadata\n",
    "with open('sample_data/cases_metadata.json', 'r') as f:\n",
    "    demo_cases = json.load(f)\n",
    "\n",
    "print(f\"ğŸ“¦ Loaded {len(demo_cases)} demo cases:\")\n",
    "for case in demo_cases:\n",
    "    print(f\"   â€¢ Case {case['case_num']}: {case['diagnosis']}\")\n",
    "\n",
    "# Select first case (Basal Cell Carcinoma)\n",
    "case = demo_cases[0]\n",
    "\n",
    "# Extract paths and metadata\n",
    "SAMPLE_CLINICAL_IMG = f'sample_data/{case[\"clinical_img\"]}'\n",
    "SAMPLE_DERMOSCOPIC_IMG = f'sample_data/{case[\"dermoscopic_img\"]}'\n",
    "metadata = case['metadata']\n",
    "\n",
    "print(f\"\\nğŸ” Running inference on: {case['diagnosis']}\")\n",
    "print(f\"   Clinical: {SAMPLE_CLINICAL_IMG}\")\n",
    "print(f\"   Dermoscopic: {SAMPLE_DERMOSCOPIC_IMG}\")\n",
    "print(f\"   Metadata: {metadata}\")\n",
    "print(f\"   Ground Truth Features: {case['clinical_features']}\")\n",
    "\n",
    "# Run inference\n",
    "results = predict(SAMPLE_CLINICAL_IMG, SAMPLE_DERMOSCOPIC_IMG, metadata)\n",
    "\n",
    "# Visualize diagnosis prediction\n",
    "fig = visualize_prediction(results, task='diag')\n",
    "plt.show()\n",
    "\n",
    "# Visualize all tasks\n",
    "fig_all = visualize_all_tasks(results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931343ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on all demo cases\n",
    "for idx, case in enumerate(demo_cases):\n",
    "    clinical_path = f'sample_data/{case[\"clinical_img\"]}'\n",
    "    dermo_path = f'sample_data/{case[\"dermoscopic_img\"]}'\n",
    "    \n",
    "    results = predict(clinical_path, dermo_path, case['metadata'])\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Case {case['case_num']} ({idx+1}/{len(demo_cases)}): {case['diagnosis']}\")\n",
    "    print(f\"Patient: {case['metadata']['sex']}, {case['metadata']['age_group']}\")\n",
    "    print(f\"Location: {case['metadata']['location']}, Elevation: {case['metadata']['elevation']}\")\n",
    "    print(f\"Ground Truth Features:\")\n",
    "    for feature, value in case['clinical_features'].items():\n",
    "        if value not in ['unknown', 'absent']:\n",
    "            print(f\"   â€¢ {feature}: {value}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Visualize diagnosis for this case\n",
    "    visualize_prediction(results, task='diag')\n",
    "    plt.show()\n",
    "    \n",
    "    # Optionally visualize all tasks (comment out if too many plots)\n",
    "    # visualize_all_tasks(results)\n",
    "    # plt.show()\n",
    "    \n",
    "    # Compare ensemble prediction with ground truth diagnosis\n",
    "    pred_idx = results['predictions']['diag']['ensemble']['prediction']\n",
    "    pred_label = DIAGNOSIS_LABELS[pred_idx]\n",
    "    pred_conf = results['predictions']['diag']['ensemble']['probabilities'][pred_idx]\n",
    "    \n",
    "    match = \"âœ“ CORRECT\" if pred_label == case['diagnosis'] else \"âœ— INCORRECT\"\n",
    "    print(f\"\\nğŸ“Š Prediction: {pred_label} ({pred_conf:.1%}) | Ground Truth: {case['diagnosis']} {match}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370f92d5",
   "metadata": {},
   "source": [
    "### 7.1 Run Inference on All Demo Cases\n",
    "\n",
    "Compare predictions across different diagnoses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081571c9",
   "metadata": {},
   "source": [
    "## 8. Interactive Widget (Optional)\n",
    "\n",
    "Create an interactive interface for uploading images and getting predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf8e6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, clear_output\n",
    "    \n",
    "    # Create widgets\n",
    "    elevation_dropdown = widgets.Dropdown(options=['flat', 'raised', 'unknown'], value='unknown', description='Elevation:')\n",
    "    sex_dropdown = widgets.Dropdown(options=['male', 'female', 'unknown'], value='unknown', description='Sex:')\n",
    "    location_dropdown = widgets.Dropdown(\n",
    "        options=['back', 'lower extremity', 'abdomen', 'face', 'chest', 'upper extremity', 'scalp', 'neck', 'unknown'],\n",
    "        value='unknown', description='Location:'\n",
    "    )\n",
    "    lesion_dropdown = widgets.Dropdown(options=['typical', 'atypical'], value='typical', description='Lesion Type:')\n",
    "    age_dropdown = widgets.Dropdown(options=['<30', '30-50', '>50'], value='30-50', description='Age Group:')\n",
    "    \n",
    "    task_dropdown = widgets.Dropdown(\n",
    "        options=list(TASK_LABELS.keys()),\n",
    "        value='diag',\n",
    "        description='Task:'\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Interactive widgets ready! (Note: File upload requires manual path specification)\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"âš ï¸ ipywidgets not available. Install with: pip install ipywidgets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89767993",
   "metadata": {},
   "source": [
    "## 9. Model Interpretation: Branch Analysis\n",
    "\n",
    "Analyze how each branch (Clinical, Dermoscopic, Combined, Meta-Combined) contributes to the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1cfba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_branch_contributions(results, task='diag'):\n",
    "    \"\"\"\n",
    "    Analyze and visualize branch contributions to ensemble prediction.\n",
    "    \"\"\"\n",
    "    pred_data = results['predictions'][task]\n",
    "    branches = pred_data['branches']\n",
    "    ensemble = pred_data['ensemble']\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    fig.suptitle(f'Branch-Level Analysis: {TASK_LABELS[task]}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    branch_names = list(branches.keys())\n",
    "    colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
    "    \n",
    "    for idx, (branch_name, ax, color) in enumerate(zip(branch_names, axes, colors)):\n",
    "        branch_data = branches[branch_name]\n",
    "        probs = branch_data['probabilities']\n",
    "        pred = branch_data['prediction']\n",
    "        \n",
    "        # Plot probabilities\n",
    "        bars = ax.bar(range(len(probs)), probs, color=color, alpha=0.7, edgecolor='black')\n",
    "        bars[pred].set_alpha(1.0)\n",
    "        bars[pred].set_linewidth(2)\n",
    "        \n",
    "        ax.set_title(f'{branch_name}\\nPred: {pred} ({probs[pred]:.3f})', fontweight='bold')\n",
    "        ax.set_xlabel('Class')\n",
    "        ax.set_ylabel('Probability')\n",
    "        ax.set_ylim([0, 1])\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add entropy indicator\n",
    "        entropy = -np.sum(probs * np.log(probs + 1e-10))\n",
    "        ax.text(0.95, 0.95, f'Entropy: {entropy:.3f}',\n",
    "               transform=ax.transAxes, ha='right', va='top',\n",
    "               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Run analysis on previous results\n",
    "if 'results' in locals():\n",
    "    fig = analyze_branch_contributions(results, task='diag')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Run inference first (Section 7) to generate results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ba2bac",
   "metadata": {},
   "source": [
    "## 10. Entropy-Weighted Ensemble Explanation\n",
    "\n",
    "Demonstrate how the model uses entropy (uncertainty) to weight different branches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab2e9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_entropy_weighting(results, task='diag'):\n",
    "    \"\"\"\n",
    "    Visualize how entropy affects branch weighting in the ensemble.\n",
    "    \"\"\"\n",
    "    pred_data = results['predictions'][task]\n",
    "    branches = pred_data['branches']\n",
    "    \n",
    "    # Calculate entropy for each branch\n",
    "    branch_names = list(branches.keys())\n",
    "    entropies = []\n",
    "    confidences = []\n",
    "    \n",
    "    for branch_name in branch_names:\n",
    "        probs = branches[branch_name]['probabilities']\n",
    "        entropy = -np.sum(probs * np.log(probs + 1e-10))\n",
    "        confidence = probs.max()\n",
    "        entropies.append(entropy)\n",
    "        confidences.append(confidence)\n",
    "    \n",
    "    # Calculate weights (inverse of entropy)\n",
    "    total_entropy = sum(entropies)\n",
    "    weights = [(1 - e/total_entropy) for e in entropies]\n",
    "    weights = np.array(weights) / sum(weights)  # Normalize\n",
    "    \n",
    "    # Visualization\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    fig.suptitle(f'Entropy-Weighted Ensemble Analysis: {TASK_LABELS[task]}', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
    "    x_pos = np.arange(len(branch_names))\n",
    "    \n",
    "    # Plot 1: Entropy\n",
    "    bars1 = ax1.bar(x_pos, entropies, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax1.set_ylabel('Entropy (Uncertainty)', fontweight='bold')\n",
    "    ax1.set_title('Branch Uncertainty')\n",
    "    ax1.set_xticks(x_pos)\n",
    "    ax1.set_xticklabels(branch_names, rotation=15, ha='right')\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Weights\n",
    "    bars2 = ax2.bar(x_pos, weights, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax2.set_ylabel('Weight', fontweight='bold')\n",
    "    ax2.set_title('Ensemble Weights (Lower Entropy â†’ Higher Weight)')\n",
    "    ax2.set_xticks(x_pos)\n",
    "    ax2.set_xticklabels(branch_names, rotation=15, ha='right')\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, weight in zip(bars2, weights):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{weight:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Plot 3: Confidence\n",
    "    bars3 = ax3.bar(x_pos, confidences, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax3.set_ylabel('Max Probability', fontweight='bold')\n",
    "    ax3.set_title('Branch Confidence')\n",
    "    ax3.set_xticks(x_pos)\n",
    "    ax3.set_xticklabels(branch_names, rotation=15, ha='right')\n",
    "    ax3.set_ylim([0, 1])\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Print explanation\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ENTROPY-WEIGHTED ENSEMBLE EXPLANATION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nTask: {TASK_LABELS[task]}\\n\")\n",
    "    for i, name in enumerate(branch_names):\n",
    "        print(f\"{name:15s} | Entropy: {entropies[i]:.3f} | Weight: {weights[i]:.3f} | Confidence: {confidences[i]:.3f}\")\n",
    "    print(\"\\nğŸ’¡ Lower entropy = More confident = Higher weight in ensemble\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Run analysis\n",
    "if 'results' in locals():\n",
    "    fig = visualize_entropy_weighting(results, task='diag')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Run inference first (Section 7) to generate results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8a3e4d",
   "metadata": {},
   "source": [
    "## 11. Model Performance Summary\n",
    "\n",
    "Display key performance metrics from training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bc9672",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_summary = \"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘           DermFormer Performance Summary                  â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘  Dataset:                Derm7pt                          â•‘\n",
    "â•‘  Training Cases:         413                              â•‘\n",
    "â•‘  Validation Cases:       203                              â•‘\n",
    "â•‘  Test Cases:             395                              â•‘\n",
    "â•‘                                                           â•‘\n",
    "â•‘  Best Epoch:             97                               â•‘\n",
    "â•‘  Total Epochs:           147 (early stopping)             â•‘\n",
    "â•‘  Training Time:          ~42 minutes                      â•‘\n",
    "â•‘                                                           â•‘\n",
    "â•‘  Final Validation Acc:   74.08%                           â•‘\n",
    "â•‘  Test Accuracy:          76.68%                           â•‘\n",
    "â•‘                                                           â•‘\n",
    "â•‘  Model Size:             211 MB                           â•‘\n",
    "â•‘  Parameters:             ~52M trainable                   â•‘\n",
    "â•‘                                                           â•‘\n",
    "â•‘  Architecture:           Multi-Modal NeST + TabTransformerâ•‘\n",
    "â•‘  Key Innovation:         Entropy-weighted ensemble        â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\"\n",
    "\n",
    "print(performance_summary)\n",
    "\n",
    "# Model architecture diagram\n",
    "architecture_flow = \"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                   DermFormer Architecture                   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "    Clinical Image          Dermoscopic Image      Metadata\n",
    "         â”‚                         â”‚                  â”‚\n",
    "         â–¼                         â–¼                  â–¼\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚ NeST-CLI â”‚              â”‚ NeST-DER â”‚       â”‚   Tab    â”‚\n",
    "   â”‚ Encoder  â”‚              â”‚ Encoder  â”‚       â”‚Transform â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚                         â”‚                  â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚\n",
    "                    â–¼                                 â”‚\n",
    "            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚\n",
    "            â”‚ Context Nest  â”‚                         â”‚\n",
    "            â”‚  (Combined)   â”‚                         â”‚\n",
    "            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚\n",
    "                    â”‚                                 â”‚\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚\n",
    "         â–¼                     â–¼                     â–¼\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚  Cross   â”‚         â”‚  Cross   â”‚         â”‚  Fusion  â”‚\n",
    "   â”‚Attention â”‚         â”‚Attention â”‚         â”‚  Layer   â”‚\n",
    "   â”‚ CLI-CTX  â”‚         â”‚ DER-CTX  â”‚         â”‚          â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚                     â”‚                     â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                    â–¼\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "         â”‚ Entropy-Weighted   â”‚\n",
    "         â”‚ Ensemble Voting    â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                    â”‚\n",
    "                    â–¼\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "         â”‚  8 Task Outputs:   â”‚\n",
    "         â”‚  â€¢ Diagnosis       â”‚\n",
    "         â”‚  â€¢ Pigment Network â”‚\n",
    "         â”‚  â€¢ Blue-White Veil â”‚\n",
    "         â”‚  â€¢ Vascular Struct â”‚\n",
    "         â”‚  â€¢ Pigmentation    â”‚\n",
    "         â”‚  â€¢ Streaks         â”‚\n",
    "         â”‚  â€¢ Dots & Globules â”‚\n",
    "         â”‚  â€¢ Regression      â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\"\n",
    "\n",
    "print(architecture_flow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc57fc1e",
   "metadata": {},
   "source": [
    "## 12. Conclusion and Key Takeaways\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "### âœ… **Technical Skills Showcased**\n",
    "- Deep learning model implementation and training\n",
    "- Multi-modal data fusion and cross-attention mechanisms\n",
    "- Ensemble learning with uncertainty quantification\n",
    "- Medical image analysis and computer vision\n",
    "- PyTorch optimization and GPU acceleration\n",
    "\n",
    "### ğŸ”¬ **Novel Contributions**\n",
    "- Entropy-weighted ensemble voting for robust predictions\n",
    "- Cross-attention fusion of clinical, dermoscopic, and metadata\n",
    "- Multi-task learning for comprehensive lesion characterization\n",
    "\n",
    "### ğŸ“Š **Results**\n",
    "- **76.68% test accuracy** on Derm7pt benchmark\n",
    "- Interpretable predictions with branch-level analysis\n",
    "- Production-ready model with efficient inference\n",
    "\n",
    "### ğŸ¯ **Applications**\n",
    "- Clinical decision support for dermatologists\n",
    "- Teledermatology screening\n",
    "- Educational tool for medical training\n",
    "- Research platform for multi-modal fusion studies\n",
    "\n",
    "---\n",
    "\n",
    "**Author**: Matthew Cockayne  \n",
    "**Contact**: [Your Email/Website]  \n",
    "**GitHub**: [Your GitHub]  \n",
    "**Date**: November 2025  \n",
    "\n",
    "ğŸ“ *This work is part of my PhD research portfolio demonstrating expertise in deep learning, medical imaging, and production ML systems.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dermformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
